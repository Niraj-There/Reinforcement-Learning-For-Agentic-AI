{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cda2856d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created Taxi-v3 environment using Gymnasium\n",
      "Environment Info:\n",
      "State space size: 500\n",
      "Action space size: 6\n",
      "Q-table shape: (500, 6)\n",
      "\n",
      "--- Starting Training ---\n",
      "Episode: 5000/25000 | Epsilon: 0.8020\n",
      "Episode: 5000/25000 | Epsilon: 0.8020\n",
      "Episode: 10000/25000 | Epsilon: 0.6040\n",
      "Episode: 10000/25000 | Epsilon: 0.6040\n",
      "Episode: 15000/25000 | Epsilon: 0.4060\n",
      "Episode: 15000/25000 | Epsilon: 0.4060\n",
      "Episode: 20000/25000 | Epsilon: 0.2080\n",
      "Episode: 20000/25000 | Epsilon: 0.2080\n",
      "Episode: 25000/25000 | Epsilon: 0.0100\n",
      "--- Training Finished in 41.38 seconds ---\n",
      "\n",
      "--- Evaluating Trained Agent ---\n",
      "Results after 100 evaluation episodes:\n",
      "Average timesteps per episode: 12.99\n",
      "Average penalties per episode: 0.00\n",
      "\n",
      "--- Visualizing a Single Episode ---\n",
      "Showing step-by-step agent behavior with the trained policy:\n",
      "\n",
      "Initial State: 371\n",
      "Initial Environment:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "\n",
      "--- Step 1 ---\n",
      "Action: North | New State: 271 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "\n",
      "--- Step 2 ---\n",
      "Action: West | New State: 251 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "\n",
      "--- Step 3 ---\n",
      "Action: West | New State: 231 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "\n",
      "--- Step 4 ---\n",
      "Action: West | New State: 211 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "\n",
      "--- Step 5 ---\n",
      "Action: South | New State: 311 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 6 ---\n",
      "Action: South | New State: 411 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 7 ---\n",
      "Action: Pickup | New State: 419 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n",
      "\n",
      "--- Step 8 ---\n",
      "Action: North | New State: 319 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "\n",
      "--- Step 9 ---\n",
      "Action: North | New State: 219 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "\n",
      "--- Step 10 ---\n",
      "Action: East | New State: 239 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "\n",
      "--- Step 11 ---\n",
      "Action: East | New State: 259 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "\n",
      "--- Step 12 ---\n",
      "Action: East | New State: 279 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "\n",
      "--- Step 13 ---\n",
      "Action: South | New State: 379 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 14 ---\n",
      "Action: South | New State: 479 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 15 ---\n",
      "Action: Dropoff | New State: 475 | Reward: 20\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "\n",
      "🎉 Episode Completed Successfully!\n",
      "Total Steps: 15\n",
      "Final Reward: 20\n",
      "✅ Passenger successfully picked up and dropped off!\n",
      "\n",
      "--- Simulation Complete ---\n",
      "The agent has been trained using Q-Learning algorithm.\n",
      "Training episodes: 25000\n",
      "Final exploration rate (epsilon): 0.0100\n",
      "Episode: 25000/25000 | Epsilon: 0.0100\n",
      "--- Training Finished in 41.38 seconds ---\n",
      "\n",
      "--- Evaluating Trained Agent ---\n",
      "Results after 100 evaluation episodes:\n",
      "Average timesteps per episode: 12.99\n",
      "Average penalties per episode: 0.00\n",
      "\n",
      "--- Visualizing a Single Episode ---\n",
      "Showing step-by-step agent behavior with the trained policy:\n",
      "\n",
      "Initial State: 371\n",
      "Initial Environment:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "\n",
      "\n",
      "--- Step 1 ---\n",
      "Action: North | New State: 271 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "\n",
      "--- Step 2 ---\n",
      "Action: West | New State: 251 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "\n",
      "--- Step 3 ---\n",
      "Action: West | New State: 231 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "\n",
      "--- Step 4 ---\n",
      "Action: West | New State: 211 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "\n",
      "\n",
      "--- Step 5 ---\n",
      "Action: South | New State: 311 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 6 ---\n",
      "Action: South | New State: 411 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[34;1m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 7 ---\n",
      "Action: Pickup | New State: 419 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[42mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "\n",
      "\n",
      "--- Step 8 ---\n",
      "Action: North | New State: 319 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[42m_\u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "\n",
      "--- Step 9 ---\n",
      "Action: North | New State: 219 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "|\u001b[42m_\u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "\n",
      "\n",
      "--- Step 10 ---\n",
      "Action: East | New State: 239 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| :\u001b[42m_\u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "\n",
      "--- Step 11 ---\n",
      "Action: East | New State: 259 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : :\u001b[42m_\u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "\n",
      "--- Step 12 ---\n",
      "Action: East | New State: 279 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : :\u001b[42m_\u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "\n",
      "\n",
      "--- Step 13 ---\n",
      "Action: South | New State: 379 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[42m_\u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 14 ---\n",
      "Action: South | New State: 479 | Reward: -1\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[42mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "\n",
      "\n",
      "--- Step 15 ---\n",
      "Action: Dropoff | New State: 475 | Reward: 20\n",
      "Environment after action:\n",
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[34;1m\u001b[43mB\u001b[0m\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "\n",
      "\n",
      "🎉 Episode Completed Successfully!\n",
      "Total Steps: 15\n",
      "Final Reward: 20\n",
      "✅ Passenger successfully picked up and dropped off!\n",
      "\n",
      "--- Simulation Complete ---\n",
      "The agent has been trained using Q-Learning algorithm.\n",
      "Training episodes: 25000\n",
      "Final exploration rate (epsilon): 0.0100\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "\n",
    "def clear_console():\n",
    "    \"\"\"Clears the console screen.\"\"\"\n",
    "    os.system('cls' if os.name == 'nt' else 'clear')\n",
    "\n",
    "# --- Environment Setup ---\n",
    "# Create the Taxi-v3 environment.\n",
    "# 'render_mode=\"ansi\"' returns a string for rendering, which is useful for non-GUI environments.\n",
    "try:\n",
    "    env = gym.make(\"Taxi-v3\", render_mode=\"ansi\")\n",
    "    print(\"Successfully created Taxi-v3 environment using Gymnasium\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: Taxi-v3 environment not found. {e}\")\n",
    "    print(\"Please make sure you have 'gymnasium' and its toy_text environments installed.\")\n",
    "    print(\"You can install it using: pip install gymnasium[toy_text]\")\n",
    "    exit()\n",
    "\n",
    "env.reset()\n",
    "\n",
    "# --- Q-Learning Algorithm Implementation ---\n",
    "\n",
    "# Initialize the Q-table with zeros.\n",
    "# The size is (number of states x number of actions).\n",
    "# Taxi-v3 has 500 states and 6 actions.\n",
    "state_space_size = env.observation_space.n\n",
    "action_space_size = env.action_space.n\n",
    "q_table = np.zeros((state_space_size, action_space_size))\n",
    "\n",
    "print(f\"Environment Info:\")\n",
    "print(f\"State space size: {state_space_size}\")\n",
    "print(f\"Action space size: {action_space_size}\")\n",
    "print(f\"Q-table shape: {q_table.shape}\")\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "num_episodes = 25000         # Total episodes for training\n",
    "max_steps_per_episode = 100  # Max steps per episode to prevent infinite loops\n",
    "\n",
    "learning_rate = 0.1          # Alpha: How much we update Q-values based on new info\n",
    "discount_rate = 0.99         # Gamma: Importance of future rewards\n",
    "\n",
    "# --- Exploration-Exploitation Parameters ---\n",
    "epsilon = 1.0                # Initial exploration rate\n",
    "max_epsilon = 1.0            # Maximum exploration rate\n",
    "min_epsilon = 0.01           # Minimum exploration rate\n",
    "# The decay rate is chosen to reduce epsilon over the episodes.\n",
    "epsilon_decay_rate = (max_epsilon - min_epsilon) / num_episodes\n",
    "\n",
    "# --- Training the Agent ---\n",
    "print(\"\\n--- Starting Training ---\")\n",
    "training_start_time = time.time()\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    # Reset the environment to a new random state for each episode\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    for step in range(max_steps_per_episode):\n",
    "        # Epsilon-Greedy Policy: Decide whether to explore or exploit\n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            action = env.action_space.sample()  # Explore: choose a random action\n",
    "        else:\n",
    "            action = np.argmax(q_table[state, :]) # Exploit: choose the best action from Q-table\n",
    "\n",
    "        # Take the action and observe the outcome\n",
    "        new_state, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        # Update the Q-table using the Bellman equation\n",
    "        q_table[state, action] = q_table[state, action] * (1 - learning_rate) + \\\n",
    "            learning_rate * (reward + discount_rate * np.max(q_table[new_state, :]))\n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "        if done or truncated:\n",
    "            break\n",
    "            \n",
    "    # Decay epsilon after each episode\n",
    "    epsilon = max(min_epsilon, epsilon - epsilon_decay_rate)\n",
    "    \n",
    "    if (episode + 1) % 5000 == 0:\n",
    "        print(f\"Episode: {episode + 1}/{num_episodes} | Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "training_end_time = time.time()\n",
    "print(f\"--- Training Finished in {training_end_time - training_start_time:.2f} seconds ---\")\n",
    "\n",
    "\n",
    "# --- Evaluating the Trained Agent ---\n",
    "print(\"\\n--- Evaluating Trained Agent ---\")\n",
    "total_epochs, total_penalties = 0, 0\n",
    "num_eval_episodes = 100\n",
    "\n",
    "for _ in range(num_eval_episodes):\n",
    "    state, info = env.reset()\n",
    "    epochs, penalties = 0, 0\n",
    "    done = False\n",
    "    truncated = False\n",
    "    \n",
    "    while not done and not truncated:\n",
    "        # We always exploit the learned policy during evaluation (no exploration)\n",
    "        action = np.argmax(q_table[state, :])\n",
    "        state, reward, done, truncated, info = env.step(action)\n",
    "\n",
    "        if reward == -10: # -10 is the penalty for illegal pickup/dropoff\n",
    "            penalties += 1\n",
    "        \n",
    "        epochs += 1\n",
    "        if epochs >= max_steps_per_episode: # Safeguard\n",
    "             break\n",
    "\n",
    "    total_penalties += penalties\n",
    "    total_epochs += epochs\n",
    "\n",
    "print(f\"Results after {num_eval_episodes} evaluation episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / num_eval_episodes:.2f}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / num_eval_episodes:.2f}\")\n",
    "\n",
    "\n",
    "# --- Visualizing an episode ---\n",
    "print(\"\\n--- Visualizing a Single Episode ---\")\n",
    "print(\"Showing step-by-step agent behavior with the trained policy:\")\n",
    "\n",
    "state, info = env.reset()\n",
    "done = False\n",
    "truncated = False\n",
    "time_step = 0\n",
    "action_map = {0: \"South\", 1: \"North\", 2: \"East\", 3: \"West\", 4: \"Pickup\", 5: \"Dropoff\"}\n",
    "\n",
    "print(f\"\\nInitial State: {state}\")\n",
    "print(\"Initial Environment:\")\n",
    "print(env.render())\n",
    "\n",
    "while not done and not truncated and time_step < max_steps_per_episode:\n",
    "    print(f\"\\n--- Step {time_step + 1} ---\")\n",
    "    \n",
    "    # Choose the best action from the Q-table\n",
    "    action = np.argmax(q_table[state, :])\n",
    "    \n",
    "    # Take action and get new state\n",
    "    state, reward, done, truncated, info = env.step(action)\n",
    "    \n",
    "    print(f\"Action: {action_map[action]} | New State: {state} | Reward: {reward}\")\n",
    "    print(\"Environment after action:\")\n",
    "    print(env.render())\n",
    "    \n",
    "    time_step += 1\n",
    "    \n",
    "    if done:\n",
    "        print(f\"\\n🎉 Episode Completed Successfully!\")\n",
    "        print(f\"Total Steps: {time_step}\")\n",
    "        print(f\"Final Reward: {reward}\")\n",
    "        if reward == 20:\n",
    "            print(\"✅ Passenger successfully picked up and dropped off!\")\n",
    "        break\n",
    "    elif truncated:\n",
    "        print(f\"\\n⚠️ Episode truncated after {time_step} steps\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n--- Simulation Complete ---\")\n",
    "print(f\"The agent has been trained using Q-Learning algorithm.\")\n",
    "print(f\"Training episodes: {num_episodes}\")\n",
    "print(f\"Final exploration rate (epsilon): {epsilon:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
